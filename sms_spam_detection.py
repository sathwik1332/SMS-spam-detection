# -*- coding: utf-8 -*-
"""SMS spam detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zGR9765X4sfYEtidEODKCHNoCc8DxtJ2
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
dataset = pd.read_csv("SMSSpamCollection", sep = '\t', names = ['label', 'messages'])
dataset

dataset.info()

dataset.describe()

dataset['label'] = dataset['label'].map({'ham':0, 'spam':1})
dataset

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
plt.figure(figsize= (5,5))
sns.countplot(x = 'label',hue = 'label', data = dataset)
plt.title("Countplot for Spam vs Ham as imbalanced data ")
plt.xlabel('Is the SMS spam?')
plt.ylabel('Count')
plt.show()

#Handling the imbalanced dataset using oversampling
only_spam = dataset[dataset['label'] == 1]
only_spam

print(f"No of spam SMS: ", len(only_spam))
print(f'No of ham SMS: ', len(dataset) - len(only_spam))

count = int((dataset.shape[0] - only_spam.shape[0]) / only_spam.shape[0])
count

for i in range(0, count-1):
    dataset = pd.concat([dataset, only_spam])
dataset.shape

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
plt.figure(figsize= (5,5))
sns.countplot(x = 'label', hue = 'label' ,data = dataset)
plt.title("Countplot for Spam vs Ham as balanced data")
plt.xlabel('Is the SMS spam?')
plt.ylabel('Count')
plt.show()

#creating a new feature called word_count
dataset['word_count'] = dataset['messages'].apply(lambda x: len(x.split()))
dataset

plt.figure(figsize = (10,4))

#(1,1)
plt.subplot(1,2,1)
sns.histplot(dataset[dataset['label'] == 0].word_count,color = '#EB5E13', kde = True)
plt.title("Distribution of word count for Ham SMS")

#(1,2)
plt.subplot(1,2,2)
sns.histplot(dataset[dataset['label'] == 1].word_count, color = '#91902A', kde = True)
plt.title("Distribution of word count for Spam SMS")

plt.tight_layout()
plt.show()

#creating new feature of containing currency symbols
def currency(data):
  currency_symbols = ['$', '₹', '€','¥', '£']
  for i in currency_symbols:
    if i in data:
      return 1
  return 0

dataset['contains_currency_symbols'] = dataset['messages'].apply(currency)
dataset

#countplot for contains currency symbols
plt.figure(figsize=(5,5))
sns.countplot(x ='contains_currency_symbols', hue = 'label', data = dataset)
plt.xlabel('Does the SMS contains any currency symbols?')
plt.ylabel('Count')
plt.title('Countplot for containing curency symbols')
plt.legend(labels = ['ham', 'spam'] )
plt.show()

#Creating a new feature of conataing numbers
def number(data):
  for i in data:
    if ord(i) >= 48 and ord(i) <= 57:
      return 1
  return 0

dataset['contains_number'] = dataset['messages'].apply(number)
dataset

#countplot for containing numbers
plt.figure(figsize=(5,5))
sns.countplot(x ='contains_number', hue = 'label', data = dataset)
plt.xlabel('Does the SMS contains any numbers?')
plt.ylabel('Count')
plt.title('Countplot for containing numbers')
plt.legend(labels = ['ham', 'spam'] )
plt.show()

#Data cleaning
import nltk
import re
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

corpus = []
wnl = WordNetLemmatizer()

for sms in list(dataset.messages):
  messages = re.sub(pattern = '[^a-zA-Z]', repl=" ", string = sms)   #Filtering out special charecters and numbers
  messages = messages.lower()
  words = messages.split()   #Tokenizing
  filtered_words = [word for word in words if word not in set(stopwords.words('english'))]
  lem_words = [wnl.lemmatize(word) for word in filtered_words]
  messages = ' '.join(lem_words)

  corpus.append(messages)

corpus

#Creating the bag of words model
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(max_features = 500)
vectors = tfidf.fit_transform(corpus).toarray()
feature_names = tfidf.get_feature_names_out()

import pandas as pd
x = pd.DataFrame(vectors, columns = feature_names)
y = dataset['label']

from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import classification_report, confusion_matrix

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

x_test

#naive bays model
from sklearn.naive_bayes import MultinomialNB
mnb = MultinomialNB()
cv = cross_val_score(mnb, x, y, scoring = 'f1', cv = 10)
print(round(cv.mean(), 3))
print(round(cv.std(), 3))

mnb.fit(x_train, y_train)
y_pred = mnb.predict(x_test)

print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
cm

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(5,5))
axis_labels = ['ham', 'spam']
sns.heatmap(data = cm, xticklabels = axis_labels, yticklabels = axis_labels, cmap = 'Blues', annot = True, cbar_kws = {'shrink':0.5}, fmt = 'g')
plt.title('Confusion matrix of multinomial Naive bayes model')
plt.xlabel('Actual values')
plt.ylabel('Predicted values')
plt.show()

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
cv1 = cross_val_score(dt, x, y, cv = 10, scoring = 'f1')
print(round(cv1.mean(), 3))
print(round(cv1.std(), 3))

dt.fit(x_train, y_train)
y_pred1 = dt.predict(x_test)

print(classification_report(y_test, y_pred1))

cm1 = confusion_matrix(y_test, y_pred1)
cm1

plt.figure(figsize=(5,5))
axis_labels = ['ham', 'spam']
sns.heatmap(data = cm1, xticklabels = axis_labels, yticklabels = axis_labels, cmap = 'Blues', annot = True, cbar_kws = {'shrink':0.5}, fmt = 'g')
plt.title('Confusion matrix of multinomial Naive bayes model')
plt.xlabel('Actual values')
plt.ylabel('Predicted values')
plt.show()

def predict_spam(sms):
  messages = re.sub(pattern = '[^a-zA-Z]', repl=" ", string = sms)   #Filtering out special charecters and numbers
  messages = messages.lower()
  words = messages.split()   #Tokenizing
  filtered_words = [word for word in words if word not in set(stopwords.words('english'))]
  lem_words = [wnl.lemmatize(word) for word in filtered_words]
  messages = ' '.join(lem_words)
  temp = tfidf.transform([messages]).toarray()
  return dt.predict(temp)

sample_message = input(f'Enter the sample message: ')

if predict_spam(sample_message):
  print('This is a SPAM message')
else:
  print("This is a HAM(normal) message")